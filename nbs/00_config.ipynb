{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "> Define the data classes using Pydantic, making it possible to configure the chat application and do input validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Tuple\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The theme of the Gradio Chat App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AppTheme data class makes it possible to configure the _colors_ and optionally a _logo_.\n",
    "\n",
    "The logo's should be stored within the data folder of the project for easy acces. In future iterations the logo's can be retrieved from:\n",
    "- the web\n",
    "- a database\n",
    "- a datafolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AppTheme(BaseModel):\n",
    "    \"\"\"Configuration for the visual theme of the app\"\"\"\n",
    "    primary_color: str = Field(default=\"#007BFF\", description=\"Primary color for UI elements\")\n",
    "    secondary_color: str = Field(default=\"#6C757D\", description=\"Secondary color for UI elements\")\n",
    "    background_color: str = Field(default=\"#FFFFFF\", description=\"Background color\")\n",
    "    text_color: str = Field(default=\"#212529\", description=\"Main text color\")\n",
    "    logo_path: Optional[Path] = Field(default=None, description=\"Path to logo image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The configuration for the workings of the LLM chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the configuration for the LLM model to use in the `ModelConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ModelConfig(BaseModel):\n",
    "    \"\"\"Configuration for the LLM model\"\"\"\n",
    "    model_name: str = Field(..., description=\"Name or path of the model to use\")\n",
    "    provider: str = Field(default=\"huggingface\", description=\"Model provider (huggingface, openai, etc)\")\n",
    "    api_key_env_var: Optional[str] = Field(default=None, description=\"Environment variable name for API key\")\n",
    "    max_tokens: int = Field(default=1024, description=\"Maximum tokens to generate\")\n",
    "    temperature: float = Field(default=0.7, description=\"Temperature for generation\")\n",
    "    stop_sequences: List[str] = Field(default=[\"\\nUser:\", \"<|endoftext|>\"], description=\"Sequences to stop generation\")\n",
    "    \n",
    "    @property\n",
    "    def api_key(self) -> Optional[str]:\n",
    "        \"\"\"Get the API key from environment variables if specified\"\"\"\n",
    "        if self.api_key_env_var:\n",
    "            return os.environ.get(self.api_key_env_var)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the configuration for the chat implementation. Making sure the application can handle:\n",
    "- system prompt\n",
    "- context if applicable\n",
    "- a start 'user' prompt if applicable\n",
    "- user input\n",
    "\n",
    "The other settings that are available in this class can easily be infered from the description in the `ChatAppConfig` class itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ChatAppConfig(BaseModel):\n",
    "    \"\"\"Main configuration for a chat application\"\"\"\n",
    "    app_name: str = Field(..., description=\"Name of the application\")\n",
    "    description: str = Field(default=\"\", description=\"Description of the application\")\n",
    "    system_prompt: str = Field(..., description=\"System prompt for the LLM\")\n",
    "    starter_prompt: Optional[str] = Field(default=None, description=\"Initial prompt to start the conversation\")\n",
    "    context_files: List[Path] = Field(default=[], description=\"List of markdown files for additional context\")\n",
    "    model: ModelConfig\n",
    "    theme: AppTheme = Field(default_factory=AppTheme)\n",
    "    show_system_prompt: bool = Field(default=True, description=\"Whether to show system prompt in UI\")\n",
    "    show_context: bool = Field(default=True, description=\"Whether to show context in UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example configuration for a chat application could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"app_name\": \"Test App\",\n",
      "  \"description\": \"\",\n",
      "  \"system_prompt\": \"You are a helpful assistant.\",\n",
      "  \"starter_prompt\": null,\n",
      "  \"context_files\": [],\n",
      "  \"model\": {\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"provider\": \"huggingface\",\n",
      "    \"api_key_env_var\": \"TEST_VALUE\",\n",
      "    \"max_tokens\": 1024,\n",
      "    \"temperature\": 0.7,\n",
      "    \"stop_sequences\": [\n",
      "      \"\\nUser:\",\n",
      "      \"<|endoftext|>\"\n",
      "    ]\n",
      "  },\n",
      "  \"theme\": {\n",
      "    \"primary_color\": \"#007BFF\",\n",
      "    \"secondary_color\": \"#6C757D\",\n",
      "    \"background_color\": \"#FFFFFF\",\n",
      "    \"text_color\": \"#212529\",\n",
      "    \"logo_path\": null\n",
      "  },\n",
      "  \"show_system_prompt\": true,\n",
      "  \"show_context\": true\n",
      "}\n",
      "API Key available: Yes\n"
     ]
    }
   ],
   "source": [
    "test_config = ChatAppConfig(\n",
    "    app_name=\"Test App\",\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    model=ModelConfig(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        api_key_env_var=\"TEST_VALUE\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(test_config.model_dump_json(indent=2))\n",
    "\n",
    "print(f\"API Key available: {'Yes' if test_config.model.api_key else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseChatApp:\n",
    "    \"\"\"Base class for creating configurable chat applications with Gradio\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ChatAppConfig):\n",
    "        self.config = config\n",
    "        self.chat_history = []\n",
    "        self._load_context()\n",
    "        \n",
    "    def _load_context(self) -> None:\n",
    "        \"\"\"Load context from markdown files\"\"\"\n",
    "        self.context_text = \"\"\n",
    "        for file_path in self.config.context_files:\n",
    "            if file_path.exists() and file_path.is_file():\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    self.context_text += f.read() + \"\\n\\n\"\n",
    "    \n",
    "    def format_prompt(self, message: str, chat_history: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"Format the prompt with system message, context, and chat history\"\"\"\n",
    "        # Start with system prompt\n",
    "        prompt = f\"System: {self.config.system_prompt}\\n\\n\"\n",
    "        \n",
    "        # Add context if available\n",
    "        if self.context_text:\n",
    "            # This part might need customization based on the model provider\n",
    "            prompt += f\"Additional information: {self.context_text}\\n\\n\"\n",
    "        \n",
    "        # Add chat history\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            prompt += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\\n\"\n",
    "        \n",
    "        # Add current message\n",
    "        prompt += f\"User: {message}\\nAssistant:\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
