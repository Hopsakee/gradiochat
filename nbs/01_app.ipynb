{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App logic\n",
    "\n",
    "> Contains BaseChatApp and large language model integration logic. Implements the core functionality indepent of the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Protocol, runtime_checkable, Generator, List, Dict, Any, Optional, Tuple\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "from gradiochat.config import ModelConfig, Message, ChatAppConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@runtime_checkable\n",
    "class LLMClientProtocol(Protocol):\n",
    "    \"\"\"Protocol defining the interface for LLM clients\"\"\"\n",
    "    \n",
    "    def chat_completion(self, messages: List[Message], **kwargs) -> str:\n",
    "        \"\"\"Generate a response from the LLM\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def chat_completion_stream(self, messages: List[Message], **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming response from the LLM\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HuggingFaceClient():\n",
    "    \"\"\"Client for interacting with HuggingFace models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig):\n",
    "        \"\"\"Initialize the client with model configuration\"\"\"\n",
    "        self.model_config = model_config\n",
    "\n",
    "        # Default to HF Inference API if no base URL is provided\n",
    "        base_url = model_config.api_base_url or \"https//router.huggingface.co/hf-inference/v1\"\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=model_config.api_key or \"hf_no_api_key_provided\"\n",
    "        )\n",
    "    \n",
    "    def chat_completion(self, \n",
    "            messages: List[Message], # List of messages conforming to the Message pydantic dataclass\n",
    "            **kwargs\n",
    "            ) -> str:\n",
    "        \"\"\"Generate a chat completion from the HuggingFace model\"\"\"\n",
    "        # Convert our Message objects to the format expected by the OpenAI client\n",
    "        openai_messages = [{\"role\": msg.role, \"content\": msg.content} for msg in messages]\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_config.model_name,\n",
    "            messages=openai_messages,\n",
    "            max_tokens=kwargs.get(\"max_tokens\", self.model_config.max_tokens),\n",
    "            temperature=kwargs.get(\"temperature\", self.model_config.temperature)\n",
    "        )\n",
    "\n",
    "        # Extract the generated text\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def chat_completion_stream(self, messages: List[Message], **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming chat completion\"\"\"\n",
    "        # For now, use non-streaming version as a placeholder\n",
    "        # We'll implement proper streaming later\n",
    "        result = self.chat_completion(messages, **kwargs)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_llm_client(model_config: ModelConfig) -> LLMClientProtocol:\n",
    "    \"\"\"\n",
    "    Factory function to create an LLM client based on the provider.\n",
    "    \"\"\"\n",
    "    if model_config.provider.lower() == \"huggingface\":\n",
    "        return HuggingFaceClient(model_config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider: {model_config.provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseChatApp:\n",
    "    \"\"\"Base class for creating configurable chat applications with Gradio\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ChatAppConfig):\n",
    "        \"\"\"Initialize the chat application\"\"\"\n",
    "        self.config = config\n",
    "        self.chat_history = []\n",
    "        self._load_context()\n",
    "        self.client = create_llm_client(config.model)\n",
    "        \n",
    "    def _load_context(self) -> None:\n",
    "        \"\"\"Load context from markdown files\"\"\"\n",
    "        self.context_text = \"\"\n",
    "        for file_path in self.config.context_files:\n",
    "            if file_path.exists() and file_path.is_file():\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    self.context_text += f.read() + \"\\n\\n\"\n",
    "    \n",
    "    def prepare_messages(self, user_message: str) -> List[Message]:\n",
    "        \"\"\"Prepare the messages for the LLM, including system prompt and chat history\"\"\"\n",
    "        messages = []\n",
    "        \n",
    "        # Add system message with prompt and context\n",
    "        system_content = self.config.system_prompt\n",
    "        if self.context_text:\n",
    "            system_content += f\"\\n\\nAdditional information: {self.context_text}\"\n",
    "        \n",
    "        messages.append(Message(role=\"system\", content=system_content))\n",
    "        \n",
    "        # Add chat history\n",
    "        for user_msg, assistant_msg in self.chat_history:\n",
    "            messages.append(Message(role=\"user\", content=user_msg))\n",
    "            messages.append(Message(role=\"assistant\", content=assistant_msg))\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append(Message(role=\"user\", content=user_message))\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def generate_response(self, user_message: str, **kwargs) -> str:\n",
    "        \"\"\"Generate a response to the user message\"\"\"\n",
    "        messages = self.prepare_messages(user_message)\n",
    "        return self.client.chat_completion(messages, **kwargs)\n",
    "    \n",
    "    def generate_stream(self, user_message: str, **kwargs) -> Generator[str, None, None]:\n",
    "        \"\"\"Generate a streaming response to the user message\"\"\"\n",
    "        messages = self.prepare_messages(user_message)\n",
    "        return self.client.chat_completion_stream(messages, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received: Engaging with the people around you is crucial for several reasons that align with the virtues of hu...\n",
      "\n",
      "Testing with overridden parameters:\n",
      "Response: As Aurelius Augustinus, I would encourage you to engage with the people around you for several reaso...\n"
     ]
    }
   ],
   "source": [
    "# Create a test model config\n",
    "test_config = ModelConfig(\n",
    "    # model_name=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\", # \"Qwen/QwQ-32B\" is another possibility, but with vision you need another messages format\n",
    "    provider=\"huggingface\",\n",
    "    api_key_env_var=\"HF_API_KEY\",\n",
    "    api_base_url=\"https://router.huggingface.co/hf-inference/v1\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Create the client\n",
    "client = create_llm_client(test_config)\n",
    "\n",
    "test_messages = [\n",
    "    Message(role=\"system\", content=\"You are Aurelius Augustinus, helping me to think deeply and be humble and thankfull.\"),\n",
    "    Message(role=\"user\", content=\"Why should I engage with the people around me?\")\n",
    "]\n",
    "# Test with a simple prompt\n",
    "try:\n",
    "    response = client.chat_completion(test_messages)\n",
    "    print(f\"Response received: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test with overriden parameters\n",
    "try:\n",
    "    print(\"\\nTesting with overridden parameters:\")\n",
    "    response = client.chat_completion(test_messages, max_tokens=50, temperature=0.9)\n",
    "    print(f\"Response: {response[:100]}...\")  # Show first 100 chars\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
