{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Interface\n",
    "\n",
    "> Gradio interface for the chat application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "import gradio as gr\n",
    "from typing import List, Tuple, Generator, Dict, Any, Optional\n",
    "from gradiochat.config import ChatAppConfig, ModelConfig\n",
    "from gradiochat.app import BaseChatApp\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GradioChat:\n",
    "    \"\"\"Gradio interface for the chat application\"\"\"\n",
    "    \n",
    "    def __init__(self, app: BaseChatApp):\n",
    "        \"\"\"Initialize with a configured BaseChatApp\"\"\"\n",
    "        self.app = app\n",
    "        self.interface = None\n",
    "    \n",
    "    def respond(self, message: str, chat_history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "        \"\"\"Generate a response to the user message and update chat history\"\"\"\n",
    "        # Store the current chat history in the app\n",
    "        self.app.chat_history = chat_history\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.app.generate_response(message)\n",
    "        \n",
    "        # Update chat history\n",
    "        chat_history.append((message, response))\n",
    "        \n",
    "        # Return empty message (to clear input) and updated history\n",
    "        return \"\", chat_history\n",
    "    \n",
    "    def respond_stream(self, message: str, chat_history: List[Tuple[str, str]]) -> Generator[Tuple[str, List[Tuple[str, str]]], None, None]:\n",
    "        \"\"\"Generate a streaming response to the user message\"\"\"\n",
    "        # Store the current chat history in the app\n",
    "        self.app.chat_history = chat_history\n",
    "        \n",
    "        # Add user message to history with empty assistant response\n",
    "        chat_history.append((message, \"\"))\n",
    "        \n",
    "        # Stream the response\n",
    "        accumulated_text = \"\"\n",
    "        for text_chunk in self.app.generate_stream(message):\n",
    "            accumulated_text += text_chunk\n",
    "            \n",
    "            # Update the last assistant message\n",
    "            updated_history = chat_history[:-1] + [(message, accumulated_text)]\n",
    "            \n",
    "            # Yield empty message and updated history\n",
    "            yield \"\", updated_history\n",
    "    \n",
    "    def build_interface(self) -> gr.Blocks:\n",
    "        \"\"\"Build and return the Gradio interface\"\"\"\n",
    "        with gr.Blocks(theme=self.app.config.theme) as interface:\n",
    "            with gr.Row():\n",
    "                # Left column for logo\n",
    "                with gr.Column(scale=1):\n",
    "                    if self.app.config.logo_path:\n",
    "                        gr.Image(value=self.app.config.logo_path,\n",
    "                            show_label=False,\n",
    "                            container=False,\n",
    "                            show_download_button=False,\n",
    "                            show_fullscreen_button=False,\n",
    "                            height=80,\n",
    "                            width=80)\n",
    "                    else:\n",
    "                        gr.Image(value=None,\n",
    "                            show_label=False,\n",
    "                            container=False,\n",
    "                            show_download_button=False,\n",
    "                            show_fullscreen_button=False,\n",
    "                            height=80,\n",
    "                            width=80)\n",
    "                with gr.Column(scale=4):\n",
    "                    # App title and description\n",
    "                    gr.Markdown(f\"# {self.app.config.app_name}\")\n",
    "                    if self.app.config.description:\n",
    "                        gr.Markdown(self.app.config.description)\n",
    "            \n",
    "            # Chat interface\n",
    "            chatbot = gr.Chatbot(height=500, label=\"Conversation\")\n",
    "            msg = gr.Textbox(\n",
    "                placeholder=\"Type your message here...\",\n",
    "                label=\"Your message\",\n",
    "                lines=2\n",
    "            )\n",
    "            \n",
    "            # Buttons\n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "                clear_btn = gr.ClearButton([msg, chatbot], value=\"Clear chat\")\n",
    "            \n",
    "            # System prompt and context viewer (collapsible)\n",
    "            with gr.Accordion(\"View System Information\", open=False):\n",
    "                if self.app.config.show_system_prompt:\n",
    "                    gr.Markdown(f\"### System Prompt\\n{self.app.config.system_prompt}\")\n",
    "                \n",
    "                if self.app.config.show_context and hasattr(self.app, 'context_text') and self.app.context_text:\n",
    "                    gr.Markdown(f\"### Additional Context\\n{self.app.context_text}\")\n",
    "            \n",
    "            # Set up event handlers\n",
    "            submit_btn.click(\n",
    "                self.respond,\n",
    "                inputs=[msg, chatbot],\n",
    "                outputs=[msg, chatbot]\n",
    "            )\n",
    "            \n",
    "            msg.submit(\n",
    "                self.respond,\n",
    "                inputs=[msg, chatbot],\n",
    "                outputs=[msg, chatbot]\n",
    "            )\n",
    "            \n",
    "            # Initialize with starter prompt if available\n",
    "            if self.app.config.starter_prompt:\n",
    "                chatbot.value = [(\"\", self.app.config.starter_prompt)]\n",
    "            \n",
    "            self.interface = interface\n",
    "            return interface\n",
    "    \n",
    "    def launch(self, **kwargs):\n",
    "        \"\"\"Launch the Gradio interface\"\"\"\n",
    "        if self.interface is None:\n",
    "            self.build_interface()\n",
    "        \n",
    "        return self.interface.launch(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_chat_app(config: ChatAppConfig) -> GradioChat:\n",
    "    \"\"\"Create a complete chat application from a configuration\"\"\"\n",
    "    base_app = BaseChatApp(config)\n",
    "    return GradioChat(base_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16046/152709889.py:72: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500, label=\"Conversation\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://686c1d9fdd6f6ddde6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://686c1d9fdd6f6ddde6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test cell - not for export\n",
    "themeWDODelta = gr.themes.Base(\n",
    "    primary_hue=gr.themes.Color(c100=\"#ffedd5\", c200=\"#ffddb3\", c300=\"#fdba74\", c400=\"#f29100\", c50=\"#fff7ed\", c500=\"#f97316\", c600=\"#ea580c\", c700=\"#c2410c\", c800=\"#9a3412\", c900=\"#7c2d12\", c950=\"#6c2e12\"),\n",
    "    neutral_hue=\"slate\",\n",
    "    radius_size=\"sm\",\n",
    "    font=['VivalaSansRound', 'ui-sans-serif', 'system-ui', 'sans-serif'],\n",
    ").set(\n",
    "    embed_radius='*radius_xs',\n",
    "    border_color_accent='*primary_400',\n",
    "    border_color_accent_dark='*secondary_700',\n",
    "    border_color_primary='*secondary_700',\n",
    "    border_color_primary_dark='*secondary_700',\n",
    "    color_accent='*primary_400',\n",
    "    shadow_drop='*shadow_drop_lg',\n",
    "    button_primary_background_fill='*primary_400',\n",
    "    button_primary_background_fill_dark='*primary_400',\n",
    "    button_primary_background_fill_hover='*secondary_700',\n",
    "    button_primary_background_fill_hover_dark='*secondary_700',\n",
    "    button_primary_border_color='*secondary_700',\n",
    "    button_primary_border_color_dark='*secondary_700'\n",
    ")\n",
    "\n",
    "# Create a test configuration\n",
    "test_config = ChatAppConfig(\n",
    "    app_name=\"Job Description Assistant\",\n",
    "    description=\"Chat with an AI to create better job descriptions\",\n",
    "    system_prompt=\"You are an assistant that helps users create professional job descriptions. Ask questions to gather information about the position and responsibilities.\",\n",
    "    starter_prompt=\"Hello! I'm your job description assistant. Tell me about the position you'd like to create a description for.\",\n",
    "    model=ModelConfig(\n",
    "        model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        api_key_env_var=\"HF_API_KEY\"\n",
    "    ),\n",
    "    theme=themeWDODelta,\n",
    "    logo_path=Path(\"../data/wdod_logo.svg\")\n",
    ")\n",
    "\n",
    "# Create and launch the app\n",
    "app = create_chat_app(test_config)\n",
    "app.launch(share=True)  # Set share=False if you don't want a public URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
