{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Interface\n",
    "\n",
    "> Gradio interface for the chat application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#| hide\n",
    "import gradio as gr\n",
    "from typing import List, Tuple, Generator, Dict, Any, Optional\n",
    "from gradiochat.config import ChatAppConfig\n",
    "from gradiochat.app import BaseChatApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GradioChat:\n",
    "    \"\"\"Gradio interface for the chat application\"\"\"\n",
    "    \n",
    "    def __init__(self, app: BaseChatApp):\n",
    "        \"\"\"Initialize with a configured BaseChatApp\"\"\"\n",
    "        self.app = app\n",
    "        self.interface = None\n",
    "    \n",
    "    def respond(self, message: str, chat_history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:\n",
    "        \"\"\"Generate a response to the user message and update chat history\"\"\"\n",
    "        # Store the current chat history in the app\n",
    "        self.app.chat_history = chat_history\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.app.generate_response(message)\n",
    "        \n",
    "        # Update chat history\n",
    "        chat_history.append((message, response))\n",
    "        \n",
    "        # Return empty message (to clear input) and updated history\n",
    "        return \"\", chat_history\n",
    "    \n",
    "    def respond_stream(self, message: str, chat_history: List[Tuple[str, str]]) -> Generator[Tuple[str, List[Tuple[str, str]]], None, None]:\n",
    "        \"\"\"Generate a streaming response to the user message\"\"\"\n",
    "        # Store the current chat history in the app\n",
    "        self.app.chat_history = chat_history\n",
    "        \n",
    "        # Add user message to history with empty assistant response\n",
    "        chat_history.append((message, \"\"))\n",
    "        \n",
    "        # Stream the response\n",
    "        accumulated_text = \"\"\n",
    "        for text_chunk in self.app.generate_stream(message):\n",
    "            accumulated_text += text_chunk\n",
    "            \n",
    "            # Update the last assistant message\n",
    "            updated_history = chat_history[:-1] + [(message, accumulated_text)]\n",
    "            \n",
    "            # Yield empty message and updated history\n",
    "            yield \"\", updated_history\n",
    "    \n",
    "    def build_interface(self) -> gr.Blocks:\n",
    "        \"\"\"Build and return the Gradio interface\"\"\"\n",
    "        with gr.Blocks(\n",
    "            theme=gr.themes.Base(\n",
    "                primary_hue=self.app.config.theme.primary_color,\n",
    "                secondary_hue=self.app.config.theme.secondary_color,\n",
    "                neutral_hue=self.app.config.theme.background_color,\n",
    "                # text_color=self.app.config.theme.text_color\n",
    "            )\n",
    "        ) as interface:\n",
    "            # App title and description\n",
    "            gr.Markdown(f\"# {self.app.config.app_name}\")\n",
    "            if self.app.config.description:\n",
    "                gr.Markdown(self.app.config.description)\n",
    "            \n",
    "            # Chat interface\n",
    "            chatbot = gr.Chatbot(height=500, label=\"Conversation\")\n",
    "            msg = gr.Textbox(\n",
    "                placeholder=\"Type your message here...\",\n",
    "                label=\"Your message\",\n",
    "                lines=2\n",
    "            )\n",
    "            \n",
    "            # Buttons\n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "                clear_btn = gr.ClearButton([msg, chatbot], value=\"Clear chat\")\n",
    "            \n",
    "            # System prompt and context viewer (collapsible)\n",
    "            with gr.Accordion(\"View System Information\", open=False):\n",
    "                if self.app.config.show_system_prompt:\n",
    "                    gr.Markdown(f\"### System Prompt\\n{self.app.config.system_prompt}\")\n",
    "                \n",
    "                if self.app.config.show_context and self.app.context_text:\n",
    "                    gr.Markdown(f\"### Additional Context\\n{self.app.context_text}\")\n",
    "            \n",
    "            # Set up event handlers\n",
    "            submit_btn.click(\n",
    "                self.respond,\n",
    "                inputs=[msg, chatbot],\n",
    "                outputs=[msg, chatbot]\n",
    "            )\n",
    "            \n",
    "            msg.submit(\n",
    "                self.respond,\n",
    "                inputs=[msg, chatbot],\n",
    "                outputs=[msg, chatbot]\n",
    "            )\n",
    "            \n",
    "            # Initialize with starter prompt if available\n",
    "            if self.app.config.starter_prompt:\n",
    "                chatbot.value = [(\"\", self.app.config.starter_prompt)]\n",
    "            \n",
    "            self.interface = interface\n",
    "            return interface\n",
    "    \n",
    "    def launch(self, **kwargs):\n",
    "        \"\"\"Launch the Gradio interface\"\"\"\n",
    "        if self.interface is None:\n",
    "            self.build_interface()\n",
    "        \n",
    "        return self.interface.launch(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_chat_app(config: ChatAppConfig) -> GradioChat:\n",
    "    \"\"\"Create a complete chat application from a configuration\"\"\"\n",
    "    base_app = BaseChatApp(config)\n",
    "    return GradioChat(base_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7615/2723046002.py:59: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500, label=\"Conversation\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://8b50ad2256ef097739.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8b50ad2256ef097739.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 207, in handle_request\n",
      "    raise UnsupportedProtocol(\n",
      "httpcore.UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 955, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 207, in handle_request\n",
      "    raise UnsupportedProtocol(\n",
      "httpcore.UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 955, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 207, in handle_request\n",
      "    raise UnsupportedProtocol(\n",
      "httpcore.UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 955, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2103, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1650, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_7615/2723046002.py\", line 16, in respond\n",
      "    response = self.app.generate_response(message)\n",
      "  File \"/home/jelle/code/gradiochat/src/gradiochat/app.py\", line 123, in generate_response\n",
      "    return self.client.chat_completion(messages, **kwargs)\n",
      "  File \"/home/jelle/code/gradiochat/src/gradiochat/app.py\", line 53, in chat_completion\n",
      "    completion = self.client.chat.completions.create(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 879, in create\n",
      "    return self._post(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 919, in request\n",
      "    return self._request(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 979, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1057, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 979, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1057, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/home/jelle/code/gradiochat/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 989, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "# Test cell - not for export\n",
    "from pathlib import Path\n",
    "from gradiochat.config import ChatAppConfig, ModelConfig, AppTheme\n",
    "from gradiochat.app import BaseChatApp\n",
    "# from ui import create_chat_app\n",
    "\n",
    "# Create a test configuration\n",
    "test_config = ChatAppConfig(\n",
    "    app_name=\"Job Description Assistant\",\n",
    "    description=\"Chat with an AI to create better job descriptions\",\n",
    "    system_prompt=\"You are an assistant that helps users create professional job descriptions. Ask questions to gather information about the position and responsibilities.\",\n",
    "    starter_prompt=\"Hello! I'm your job description assistant. Tell me about the position you'd like to create a description for.\",\n",
    "    model=ModelConfig(\n",
    "        model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        api_key_env_var=\"HF_API_KEY\"\n",
    "    ),\n",
    "    theme=AppTheme(\n",
    "        primary_color=\"blue\",  # Green\n",
    "        secondary_color=\"orange\"  # Blue\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create and launch the app\n",
    "app = create_chat_app(test_config)\n",
    "app.launch(share=True)  # Set share=False if you don't want a public URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
