# Configuration


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

#### Import statement

``` python
from gradiochat.config import *
```

## Some lessons and clarification about the used code

1.  `Pydantic` vs `Dataclasses`: Pydantic creates classes similar to
    Python’s dataclasses but with additional features. The key
    differences are that Pydantic provides data validation, type
    coercion, and more robust error handling. It will automatically
    validate data during initialization and conversion.
2.  Pydantic and typing: Pydantic leverages Python’s standard typing
    system but adds its own validation on top. It uses Python’s type
    hints to know what types to validate against.
3.  The “…” placeholder: The ellipsis (…) is a special value in Pydantic
    that indicates a required field. It means “this field must be
    provided when creating an instance” - there’s no default value. When
    you create a ModelConfig instance, you’ll need to provide a value
    for model_name.
4.  `@property` usage: The `@property` decorator creates a getter method
    that’s accessed like an attribute. In our case, api_key looks like a
    normal attribute but when accessed, it runs the method to retrieve
    the value from environment variables. This is a clean way to avoid
    storing sensitive information in the object itself.
5.  `Field` from `pydantic` can be used to add extra information and
    metadata to inform the reader and/or do data validation.

## The configuration for the workings of the LLM chatbot

#### ModelConfig

First the configuration for the LLM model to use in the `ModelConfig`.

------------------------------------------------------------------------

### ModelConfig

>  ModelConfig (model_name:str, provider:str='huggingface',
>                   api_key_env_var:Optional[str]=None,
>                   api_base_url:Optional[str]=None, temperature:float=0.7,
>                   max_completion_tokens:int=1024, top_p:float=0.7,
>                   top_k:int=50, frequency_penalty:float=0,
>                   stop:Optional[List[str]]=['\nUser:', '<|endoftext|>'],
>                   stream:Optional[bool]=None)

*Configuration for the LLM model*

## ModelConfig

Configuration for the LLM model

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr>
<th>Variable</th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>model_name</code></td>
<td><code>str</code></td>
<td>PydanticUndefined</td>
<td>Name or path of the model to use</td>
</tr>
<tr>
<td><code>provider</code></td>
<td><code>str</code></td>
<td>‘huggingface’</td>
<td>Model provider (huggingface, openai, etc)</td>
</tr>
<tr>
<td><code>api_key_env_var</code></td>
<td><code>Optional[str]</code></td>
<td>None</td>
<td>Environment variable name for API key</td>
</tr>
<tr>
<td><code>api_base_url</code></td>
<td><code>Optional[str]</code></td>
<td>None</td>
<td>Base URL for API reqeuest</td>
</tr>
<tr>
<td><code>temperature</code></td>
<td><code>float</code></td>
<td>0.7</td>
<td>Temperature for generation</td>
</tr>
<tr>
<td><code>max_completion_tokens</code></td>
<td><code>int</code></td>
<td>1024</td>
<td>Maximum tokens to generate</td>
</tr>
<tr>
<td><code>top_p</code></td>
<td><code>float</code></td>
<td>0.7</td>
<td>Adjust the number of choices for each predicted token [0-1]</td>
</tr>
<tr>
<td><code>top_k</code></td>
<td><code>int</code></td>
<td>50</td>
<td>Limits the number of choices for the next predicted token. Not
available for OpenAI API</td>
</tr>
<tr>
<td><code>frequency_penalty</code></td>
<td><code>float</code></td>
<td>0</td>
<td>Reduces the likelihood of repeating prompt text or getting stuck in
a loop [-2 -&gt; 2]</td>
</tr>
<tr>
<td><code>stop</code></td>
<td><code>Optional[list[str]]</code></td>
<td>[‘:’, ‘&lt;|endoftext|&gt;’]</td>
<td>Sequences to stop generation</td>
</tr>
<tr>
<td><code>stream</code></td>
<td><code>Optional[bool]</code></td>
<td>None</td>
<td>If set to true, the model response data will be streamed to the
client as it is generated using server-sent events.</td>
</tr>
</tbody>
</table>

#### Message config

Next the configuration of the Message system

------------------------------------------------------------------------

### Message

>  Message (role:Literal['system','user','assistant'], content:str)

*A message in a conversation*

## Message

A message in a conversation

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr>
<th>Variable</th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>role</code></td>
<td><code>Literal[system, user, assistant]</code></td>
<td>PydanticUndefined</td>
<td>Role of the message sender</td>
</tr>
<tr>
<td><code>content</code></td>
<td><code>str</code></td>
<td>PydanticUndefined</td>
<td>Content of the message</td>
</tr>
</tbody>
</table>

#### Chat config

Then the configuration for the chat implementation. Making sure the
application can handle: - system prompt - context if applicable - a
start ‘user’ prompt if applicable - user input

The other settings that are available in this class can easily be
infered from the description in the `ChatAppConfig` class itself.

------------------------------------------------------------------------

### ChatAppConfig

>  ChatAppConfig (app_name:str, description:str='', system_prompt:str,
>                     starter_prompt:Optional[str]=None,
>                     context_files:List[pathlib.Path]=[],
>                     model:__main__.ModelConfig, theme:Optional[Any]=None,
>                     logo_path:Optional[pathlib.Path]=None,
>                     show_system_prompt:bool=True, show_context:bool=True)

*Main configuration for a chat application*

## ChatAppConfig

Main configuration for a chat application

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr>
<th>Variable</th>
<th>Type</th>
<th>Default</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>app_name</code></td>
<td><code>str</code></td>
<td>PydanticUndefined</td>
<td>Name of the application</td>
</tr>
<tr>
<td><code>description</code></td>
<td><code>str</code></td>
<td>’’</td>
<td>Description of the application</td>
</tr>
<tr>
<td><code>system_prompt</code></td>
<td><code>str</code></td>
<td>PydanticUndefined</td>
<td>System prompt for the LLM</td>
</tr>
<tr>
<td><code>starter_prompt</code></td>
<td><code>Optional[str]</code></td>
<td>None</td>
<td>Initial prompt to start the conversation</td>
</tr>
<tr>
<td><code>context_files</code></td>
<td><code>list[Path]</code></td>
<td>[]</td>
<td>List of markdown files for additional context</td>
</tr>
<tr>
<td><code>model</code></td>
<td><code>ModelConfig</code></td>
<td>PydanticUndefined</td>
<td>(see <code>ModelConfig</code> table)</td>
</tr>
<tr>
<td><code>theme</code></td>
<td><code>Optional[Any]</code></td>
<td>None</td>
<td>Gradio theme to use</td>
</tr>
<tr>
<td><code>logo_path</code></td>
<td><code>Optional[Path]</code></td>
<td>None</td>
<td>Path to logo image</td>
</tr>
<tr>
<td><code>show_system_prompt</code></td>
<td><code>bool</code></td>
<td>True</td>
<td>Whether to show system prompt in UI</td>
</tr>
<tr>
<td><code>show_context</code></td>
<td><code>bool</code></td>
<td>True</td>
<td>Whether to show context in UI</td>
</tr>
</tbody>
</table>

An example configuration for a chat application:

``` python
# Eval set to false, because the api key is stored in .env and thus can't be found when
# nbdev_test is run
test_config = ChatAppConfig(
    app_name="Test App",
    system_prompt="You are a helpful assistant.",
    model=ModelConfig(
        model_name="gpt-3.5-turbo",
        api_key_env_var="TEST_API_KEY",
    )
)

print(test_config.model_dump_json(indent=2))

print(f"API Key available: {'Yes' if test_config.model.api_key else 'No'}")
```

    {
      "app_name": "Test App",
      "description": "",
      "system_prompt": "You are a helpful assistant.",
      "starter_prompt": null,
      "context_files": [],
      "model": {
        "model_name": "gpt-3.5-turbo",
        "provider": "huggingface",
        "api_key_env_var": "TEST_API_KEY",
        "api_base_url": null,
        "temperature": 0.7,
        "max_completion_tokens": 1024,
        "top_p": 0.7,
        "top_k": 50,
        "frequency_penalty": 0.0,
        "stop": [
          "\nUser:",
          "<|endoftext|>"
        ],
        "stream": null
      },
      "theme": null,
      "logo_path": null,
      "show_system_prompt": true,
      "show_context": true
    }
    API Key available: Yes
